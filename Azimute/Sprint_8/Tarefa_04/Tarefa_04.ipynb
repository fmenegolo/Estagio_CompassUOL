{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hYDKJWpFHOo",
        "outputId": "bfa93781-11a5-45a0-90ad-1dd04ecf8d2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l7Y1SGhEY3U"
      },
      "source": [
        "# Perguntas dessa tarefa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7PHoIrIESIW"
      },
      "source": [
        "## 1.\n",
        "\n",
        "- Inicialmente iremos preparar o ambiente, definindo o diretório onde nosso código será desenvolvido. Para este diretório iremos copiar o arquivo nomes_aleatorios.txt.\n",
        "\n",
        "- Após, em nosso script Python, devemos importar as bibliotecas necessárias:\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark import SparkContext, SQLContext\n",
        "\n",
        "- Aplicando as bibliotecas do Spark, podemos definir a Spark Session e sobre ela definir o Context para habilitar o módulo SQL\n",
        "\n",
        "spark = SparkSession \\\n",
        "\n",
        "                .builder \\\n",
        "\n",
        "                .master(\"local[*]\")\\\n",
        "\n",
        "                .appName(\"Exercicio Intro\") \\\n",
        "\n",
        "                .getOrCreate()\n",
        "\n",
        "- Nesta etapa, adicione código para ler o arquivo nomes_aleatorios.txt através do comando spark.read.csv. Carregue-o para dentro de um dataframe chamado df_nomes e, por fim, liste algumas linhas através do método show. Exemplo: df_nomes.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlEEmzN0ESId",
        "outputId": "a5dd3f92-e17c-44ea-b0d9-75a44722091b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+\n",
            "|             _c0|\n",
            "+----------------+\n",
            "|  Frances Bennet|\n",
            "|   Jamie Russell|\n",
            "|  Edward Kistler|\n",
            "|   Sheila Maurer|\n",
            "|Donald Golightly|\n",
            "+----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SQLContext\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .master(\"local[*]\")\\\n",
        "    .appName(\"Exercicio Intro\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "df_nomes = spark.read.csv('nomes_aleatorios.txt')\n",
        "\n",
        "df_nomes.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUzxXu7dESIe"
      },
      "source": [
        "## 2. \n",
        " - No Python, é possível acessar uma coluna de um objeto dataframe pelo atributo (por exemplo df_nomes.nome) ou por índice (df_nomes['nome']). Enquanto a primeira forma é conveniente para a exploração de dados interativos, você deve usar o formato de índice, pois caso algum nome de coluna não esteja de acordo seu código irá falhar.\n",
        "\n",
        " - Como não informamos no momento da leitura do arquivo, o Spark não identificou o Schema por padrão e definiu todas as colunas como string. Para ver o Schema, use o método df_nomes.printSchema().\n",
        "\n",
        "\n",
        "\n",
        " - Nesta etapa, será necessário adicionar código para renomear a coluna para Nomes, imprimir o esquema e mostrar 10 linhas do dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21ipYuCfc5Co",
        "outputId": "afbef4d9-ae62-4a20-9050-57bb04a30c28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- Nomes: string (nullable = true)\n",
            "\n",
            "+-----------------+\n",
            "|            Nomes|\n",
            "+-----------------+\n",
            "|   Frances Bennet|\n",
            "|    Jamie Russell|\n",
            "|   Edward Kistler|\n",
            "|    Sheila Maurer|\n",
            "| Donald Golightly|\n",
            "|       David Gray|\n",
            "|      Joy Bennett|\n",
            "|      Paul Kriese|\n",
            "|Berniece Ornellas|\n",
            "|    Brian Farrell|\n",
            "+-----------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_nomes.printSchema()\n",
        "\n",
        "nomesschema = \"nome STRING\"\n",
        "\n",
        "df_nomes = df_nomes.withColumnRenamed(\"_c0\", \"Nomes\")\n",
        "\n",
        "df_nomes.printSchema()\n",
        "\n",
        "df_nomes.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJP7TAXzESIe"
      },
      "source": [
        "## 3.\n",
        "- Ao dataframe (df_nomes), adicione nova coluna chamada Escolaridade e atribua para cada linha um dos três valores de forma aleatória: Fundamental, Medio ou Superior.\n",
        "\n",
        "- Para esta etapa, evite usar funções de iteração, como por exemplo: for, while, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QihqXAnjdBju"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import when, rand\n",
        "\n",
        "df_nomes = df_nomes.withColumn(\"Escolaridade\", when(rand() < 0.33, \"Fundamental\")\n",
        "                                        .when(rand() < 0.67, \"Medio\")\n",
        "                                        .otherwise(\"Superior\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LINeYV0ESIf"
      },
      "source": [
        "## 4.\n",
        "\n",
        "- Ao dataframe (df_nomes), adicione nova coluna chamada Pais e atribua para cada linha o nome de um dos 13 países da América do Sul, de forma aleatória.\n",
        "\n",
        "- Para esta etapa, evite usar funções de iteração, como por exemplo: for, while, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7c0P6AcmdtCm"
      },
      "outputs": [],
      "source": [
        "paises = [(1,'Brasil'), (2,'Argentina'), (3,'Paraguai'), (4,'Uruguai'), (5,'Bolivia'), (6,'Chile'), (7,'Equador'), (8,'Guiana Francesa'), (9,'Peru'), (10,'Colombia'), (11,'Suriname'), (12,'Venezuela'), (13,'Guiana')] \n",
        "paises_schema = \"id INT, Pais STRING\"\n",
        "paises_df = spark.createDataFrame(paises, paises_schema)\n",
        "\n",
        "df_nomes = df_nomes.withColumn(\"id_pais\", (rand()*13+1).cast(\"int\"))\n",
        "\n",
        "df_nomes = df_nomes.join(paises_df, df_nomes.id_pais == paises_df.id, \"inner\")\n",
        "\n",
        "df_nomes = df_nomes.drop(\"id_pais\", \"id\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbr_yIh9ESIg"
      },
      "source": [
        "## 5.\n",
        " - Ao dataframe (df_nomes), adicione nova coluna chamada AnoNascimento e atribua para cada linha um valor de ano entre 1945 e 2010, de forma aleatória. \n",
        " - Para esta etapa, evite usar funções de iteração, como por exemplo: for, while, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mRY-dV2cmqOT"
      },
      "outputs": [],
      "source": [
        "df_nomes = df_nomes.withColumn(\"AnoNascimento\", (rand() * (2010 - 1945) + 1945).cast(\"int\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Cs53gyZESIg"
      },
      "source": [
        "## 6. \n",
        "- Usando o método select do dataframe (df_nomes), selecione as pessoas que nasceram neste século. Armazene o resultado em outro dataframe chamado df_select e mostre 10 nomes deste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1uAcN0AqTmK",
        "outputId": "7491c58a-5705-44f3-ee9e-e5e9c0830ee6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+------------+------+-------------+\n",
            "|            Nomes|Escolaridade|  Pais|AnoNascimento|\n",
            "+-----------------+------------+------+-------------+\n",
            "|    Milton Dillon| Fundamental|Brasil|         2003|\n",
            "|        Leo Moore|       Medio|Brasil|         2006|\n",
            "|    Joseph Fenton|    Superior|Brasil|         2005|\n",
            "|    Stewart Hinds|       Medio|Brasil|         2007|\n",
            "| Margaret Bowling|       Medio|Brasil|         2009|\n",
            "|Francis Conaughty| Fundamental|Brasil|         2004|\n",
            "|   Patricia Mixon|       Medio|Brasil|         2001|\n",
            "|Peggy Quintanilla|       Medio|Brasil|         2001|\n",
            "|     Doris Hanson|       Medio|Brasil|         2002|\n",
            "|   Linda Weishaar| Fundamental|Brasil|         2002|\n",
            "+-----------------+------------+------+-------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import functions as Func\n",
        "\n",
        "df_select = df_nomes.select(\"*\").where(Func.col('AnoNascimento') >= 2000)\n",
        "\n",
        "df_select.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NHbssBtESIh"
      },
      "source": [
        "## 7. \n",
        "- Usando Spark SQL repita o processo da Pergunta 6. Lembre-se que, para trabalharmos com SparkSQL, precisamos registrar uma tabela temporária e depois executar o comando SQL. Abaixo um exemplo de como executar comandos SQL com SparkSQL:\n",
        "\n",
        " df_nomes.createOrReplaceTempView (\"pessoas\")\n",
        "\n",
        " spark.sql(\"select * from pessoas\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7anIh9ZKtsw",
        "outputId": "78a81f38-6f23-44d0-c517-8233b1e8f506"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+------------+------+-------------+\n",
            "|            Nomes|Escolaridade|  Pais|AnoNascimento|\n",
            "+-----------------+------------+------+-------------+\n",
            "|    Milton Dillon| Fundamental|Brasil|         2003|\n",
            "|        Leo Moore|       Medio|Brasil|         2006|\n",
            "|    Joseph Fenton|    Superior|Brasil|         2005|\n",
            "|    Stewart Hinds|       Medio|Brasil|         2007|\n",
            "| Margaret Bowling|       Medio|Brasil|         2009|\n",
            "|Francis Conaughty| Fundamental|Brasil|         2004|\n",
            "|   Patricia Mixon|       Medio|Brasil|         2001|\n",
            "|Peggy Quintanilla|       Medio|Brasil|         2001|\n",
            "|     Doris Hanson|       Medio|Brasil|         2002|\n",
            "|   Linda Weishaar| Fundamental|Brasil|         2002|\n",
            "+-----------------+------------+------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_nomes.createOrReplaceTempView(\"pessoas\")\n",
        "\n",
        "seculo_XX_sql = \"\"\"\n",
        "SELECT *\n",
        "FROM pessoas\n",
        "WHERE AnoNascimento >= 2000\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "spark.sql(seculo_XX_sql).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJyOQSv9ESIh"
      },
      "source": [
        "## 8.\n",
        "- Usando o método select do Dataframe df_nomes, Conte o número de pessoas que são da geração Millennials (nascidos entre 1980 e 1994) no Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPYc8XlDMHCV",
        "outputId": "ff0b4636-7010-426a-bf69-ab95b62ccb5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------+\n",
            "|    geracao|  count|\n",
            "+-----------+-------+\n",
            "|Millennials|2307976|\n",
            "+-----------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_nomes.select(Func.when((Func.col('AnoNascimento') >= 1980) & (Func.col('AnoNascimento') <= 1994), 'Millennials').otherwise('Outras geracoes').alias('geracao')).groupBy('geracao').count().filter(Func.col('geracao') == 'Millennials').show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfrZjG8zESIi"
      },
      "source": [
        "## 9.\n",
        "- Repita o processo da Pergunta 8 utilizando Spark SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV0AhNH3fey_",
        "outputId": "24a4d193-ac4b-4f74-9dd2-63540fcd9ae3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------+\n",
            "|    geracao|count(1)|\n",
            "+-----------+--------+\n",
            "|Millennials| 2307976|\n",
            "+-----------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "millennials_sql = \"\"\"\n",
        "SELECT \n",
        "    CASE \n",
        "        WHEN AnoNascimento BETWEEN 1980 AND 1994 THEN \"Millennials\"\n",
        "        ELSE \"Outras gerações\"\n",
        "    END AS geracao, \n",
        "    COUNT(*)\n",
        "FROM pessoas\n",
        "GROUP BY geracao\n",
        "HAVING geracao = \"Millennials\"\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(millennials_sql).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U60IH5YNESIi"
      },
      "source": [
        "## 10. \n",
        " - Usando Spark SQL, obtenha a quantidade de pessoas de cada país para uma das gerações abaixo. Armazene o resultado em um novo dataframe e depois mostre todas as linhas em ordem crescente de Pais, Geração e Quantidade\n",
        "\n",
        "    - Baby Boomers – nascidos entre 1944 e 1964;\n",
        "\n",
        "    - Geração X – nascidos entre 1965 e 1979;\n",
        "\n",
        "    - Millennials (Geração Y) – nascidos entre 1980 e 1994;\n",
        "\n",
        "    - Geração Z – nascidos entre 1995 e 2015."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx74JQeipeNa",
        "outputId": "d952e9c1-20ec-4663-deb7-4839fd3455a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+------------+----------+\n",
            "|           Pais|     Geracao|Quantidade|\n",
            "+---------------+------------+----------+\n",
            "|      Argentina|Baby Boomers|    236567|\n",
            "|      Argentina|   Geração X|    178229|\n",
            "|      Argentina|   Geração Y|    178498|\n",
            "|      Argentina|   Geração Z|    177422|\n",
            "|        Bolivia|Baby Boomers|    236337|\n",
            "|        Bolivia|   Geração X|    177821|\n",
            "|        Bolivia|   Geração Y|    176823|\n",
            "|        Bolivia|   Geração Z|    177842|\n",
            "|         Brasil|Baby Boomers|    237850|\n",
            "|         Brasil|   Geração X|    177511|\n",
            "|         Brasil|   Geração Y|    177482|\n",
            "|         Brasil|   Geração Z|    177327|\n",
            "|          Chile|Baby Boomers|    236254|\n",
            "|          Chile|   Geração X|    177512|\n",
            "|          Chile|   Geração Y|    177548|\n",
            "|          Chile|   Geração Z|    176886|\n",
            "|       Colombia|Baby Boomers|    235868|\n",
            "|       Colombia|   Geração X|    177659|\n",
            "|       Colombia|   Geração Y|    177086|\n",
            "|       Colombia|   Geração Z|    177130|\n",
            "|        Equador|Baby Boomers|    236616|\n",
            "|        Equador|   Geração X|    177828|\n",
            "|        Equador|   Geração Y|    177977|\n",
            "|        Equador|   Geração Z|    176925|\n",
            "|         Guiana|Baby Boomers|    236462|\n",
            "|         Guiana|   Geração X|    177515|\n",
            "|         Guiana|   Geração Y|    176747|\n",
            "|         Guiana|   Geração Z|    177692|\n",
            "|Guiana Francesa|Baby Boomers|    236787|\n",
            "|Guiana Francesa|   Geração X|    177608|\n",
            "|Guiana Francesa|   Geração Y|    177221|\n",
            "|Guiana Francesa|   Geração Z|    177366|\n",
            "|       Paraguai|Baby Boomers|    236652|\n",
            "|       Paraguai|   Geração X|    177488|\n",
            "|       Paraguai|   Geração Y|    177448|\n",
            "|       Paraguai|   Geração Z|    177912|\n",
            "|           Peru|Baby Boomers|    236421|\n",
            "|           Peru|   Geração X|    178028|\n",
            "|           Peru|   Geração Y|    177773|\n",
            "|           Peru|   Geração Z|    176226|\n",
            "|       Suriname|Baby Boomers|    237548|\n",
            "|       Suriname|   Geração X|    177820|\n",
            "|       Suriname|   Geração Y|    177657|\n",
            "|       Suriname|   Geração Z|    178181|\n",
            "|        Uruguai|Baby Boomers|    237225|\n",
            "|        Uruguai|   Geração X|    177595|\n",
            "|        Uruguai|   Geração Y|    177713|\n",
            "|        Uruguai|   Geração Z|    177708|\n",
            "|      Venezuela|Baby Boomers|    235903|\n",
            "|      Venezuela|   Geração X|    177055|\n",
            "|      Venezuela|   Geração Y|    178003|\n",
            "|      Venezuela|   Geração Z|    177248|\n",
            "+---------------+------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "consulta_sql = \"\"\"\n",
        "SELECT Pais,\n",
        "    CASE \n",
        "        WHEN (AnoNascimento BETWEEN 1944 AND 1964) THEN 'Baby Boomers'\n",
        "        WHEN (AnoNascimento BETWEEN 1965 AND 1979) THEN 'Geração X'\n",
        "        WHEN (AnoNascimento BETWEEN 1980 AND 1994) THEN 'Geração Y'\n",
        "        WHEN (AnoNascimento BETWEEN 1995 AND 2015) THEN 'Geração Z'\n",
        "        ELSE 'Outras gerações'\n",
        "    END AS Geracao,\n",
        "    COUNT(*) AS Quantidade\n",
        "FROM pessoas\n",
        "GROUP BY Pais, Geracao\n",
        "\"\"\"\n",
        "novo_df = spark.sql(consulta_sql)\n",
        "novo_df.orderBy(col(\"Pais\"), col(\"Geracao\"), col(\"Quantidade\")).show(1000)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.-1"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
