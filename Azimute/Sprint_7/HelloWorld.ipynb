{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "spark_home = \"/opt/apache-spark\"\n",
    "os.environ[\"SPARK_HOME\"] = spark_home\n",
    "\n",
    "# Add the PySpark directories to the Python path\n",
    "sys.path.insert(0, os.path.join(spark_home, \"python\"))\n",
    "sys.path.insert(0, os.path.join(spark_home, \"python\", \"lib\", \"py4j-0.10.9-src.zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'py4j'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m      3\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mPYSPARK_PYTHON\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mexecutable\n\u001b[0;32m----> 4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession\n\u001b[1;32m      6\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mmaster(\u001b[39m\"\u001b[39m\u001b[39mlocal[*]\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m      7\u001b[0m  \u001b[39m.\u001b[39mappName(\u001b[39m'\u001b[39m\u001b[39mSparkHelloWorld\u001b[39m\u001b[39m'\u001b[39m) \\\n\u001b[1;32m      8\u001b[0m  \u001b[39m.\u001b[39mgetOrCreate()\n",
      "File \u001b[0;32m/opt/apache-spark/python/pyspark/__init__.py:53\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtypes\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m cast, Any, Callable, Optional, TypeVar, Union\n\u001b[0;32m---> 53\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconf\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkConf\n\u001b[1;32m     54\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrdd\u001b[39;00m \u001b[39mimport\u001b[39;00m RDD, RDDBarrier\n\u001b[1;32m     55\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfiles\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkFiles\n",
      "File \u001b[0;32m/opt/apache-spark/python/pyspark/conf.py:23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Dict, List, Optional, Tuple, cast, overload\n\u001b[0;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpy4j\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mjava_gateway\u001b[39;00m \u001b[39mimport\u001b[39;00m JVMView, JavaObject\n\u001b[1;32m     26\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mSparkConf\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39m    Configuration for a Spark application. Used to set various Spark\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m    parameters as key-value pairs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39m    spark.home=/path\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'py4j'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    " .appName('SparkHelloWorld') \\\n",
    " .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+---+------+\n",
      "|firstname|middlename|lastname|   id|sex|salary|\n",
      "+---------+----------+--------+-----+---+------+\n",
      "|    James|          |   Smith|36636|  M|  3000|\n",
      "|  Michael|      Rose|        |40288|  M|    -1|\n",
      "|   Robert|          |Williams|42114|  M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|  F|  4000|\n",
      "|      Jen|      Mary|   Brown| null|  F|  3000|\n",
      "+---------+----------+--------+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "data = [\n",
    " (\"James\", \"\", \"Smith\", \"36636\", \"M\", 3000), \n",
    " (\"Michael\", \"Rose\", \"\", \"40288\", \"M\", -1), \n",
    " (\"Robert\", \"\", \"Williams\", \"42114\", \"M\", 4000), \n",
    " (\"Maria\", \"Anne\", \"Jones\", \"39192\", \"F\", 4000), \n",
    " (\"Jen\", \"Mary\", \"Brown\", None, \"F\", 3000)\n",
    "]\n",
    "schema = StructType([\n",
    " StructField(\"firstname\", StringType(), True), \\\n",
    " StructField(\"middlename\", StringType(), True), \\\n",
    " StructField(\"lastname\", StringType(), True), \\\n",
    " StructField(\"id\", StringType(), True), \\\n",
    " StructField(\"sex\", StringType(), True), \\\n",
    " StructField(\"salary\", IntegerType(), True) \\\n",
    " ])\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+---+------+\n",
      "|firstname|middlename|lastname|   id|sex|salary|\n",
      "+---------+----------+--------+-----+---+------+\n",
      "|    James|          |   Smith|36636|  M|  3000|\n",
      "|   Robert|          |Williams|42114|  M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|  F|  4000|\n",
      "+---------+----------+--------+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df_filtered = df.where(\n",
    " (col(\"id\").isNotNull()) & (col(\"salary\") > 0)\n",
    ")\n",
    "df_filtered.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+---+------+--------------+\n",
      "|firstname|middlename|lastname|   id|sex|salary|      fullname|\n",
      "+---------+----------+--------+-----+---+------+--------------+\n",
      "|    James|          |   Smith|36636|  M|  3000|    JamesSmith|\n",
      "|   Robert|          |Williams|42114|  M|  4000|RobertWilliams|\n",
      "|    Maria|      Anne|   Jones|39192|  F|  4000|MariaAnneJones|\n",
      "+---------+----------+--------+-----+---+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "df_full_name = df_filtered.withColumn(\n",
    " \"fullname\",\n",
    " concat(col(\"firstname\"), col(\"middlename\"), col(\"lastname\"))\n",
    ")\n",
    "df_full_name.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+---+------+----------------+\n",
      "|firstname|middlename|lastname|   id|sex|salary|        fullname|\n",
      "+---------+----------+--------+-----+---+------+----------------+\n",
      "|    James|          |   Smith|36636|  M|  3000|    James  Smith|\n",
      "|   Robert|          |Williams|42114|  M|  4000|Robert  Williams|\n",
      "|    Maria|      Anne|   Jones|39192|  F|  4000|Maria Anne Jones|\n",
      "+---------+----------+--------+-----+---+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df_full_name2 = df_filtered.withColumn(\n",
    " \"fullname\",\n",
    " concat(\n",
    " col(\"firstname\"),\n",
    " lit(\" \"),\n",
    " col(\"middlename\"),\n",
    " lit(\" \"),\n",
    " col(\"lastname\"))\n",
    ")\n",
    "df_full_name2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+---+------+----------------+\n",
      "|firstname|middlename|lastname|   id|sex|salary|        fullname|\n",
      "+---------+----------+--------+-----+---+------+----------------+\n",
      "|    James|          |   Smith|36636|  M|  3000|     James Smith|\n",
      "|   Robert|          |Williams|42114|  M|  4000| Robert Williams|\n",
      "|    Maria|      Anne|   Jones|39192|  F|  4000|Maria Anne Jones|\n",
      "+---------+----------+--------+-----+---+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered.createOrReplaceTempView(\"df\")\n",
    "df_full_name3 = spark.sql(\"\"\"\n",
    " SELECT *,\n",
    " CASE\n",
    " WHEN middlename = '' THEN concat(firstname, \" \", lastname)\n",
    " ELSE concat(firstname, \" \", middlename, \" \", lastname)\n",
    " END AS fullname\n",
    " FROM df\n",
    "\"\"\")\n",
    "df_full_name3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+---+------+-----------------+\n",
      "|firstname|middlename|lastname|   id|sex|salary|         fullname|\n",
      "+---------+----------+--------+-----+---+------+-----------------+\n",
      "|    James|          |   Smith|36636|  M|  3000|      James Smith|\n",
      "|   Robert|          |Williams|42114|  M|  4000|  Robert Williams|\n",
      "|    Maria|      Anne|   Jones|39192|  F|  4000|Maria Jones Jones|\n",
      "+---------+----------+--------+-----+---+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df_full_name4 = df_filtered.withColumn(\n",
    " \"fullname\",\n",
    " when(\n",
    " col(\"middlename\") == \"\",\n",
    " concat(\n",
    " col(\"firstname\"),\n",
    " lit(\" \"),\n",
    " col(\"lastname\")\n",
    " )\n",
    " ).otherwise(\n",
    " concat(\n",
    " col(\"firstname\"),\n",
    " lit(\" \"),\n",
    " col(\"lastname\"),\n",
    " lit(\" \"),\n",
    " col(\"lastname\")\n",
    " )\n",
    " )\n",
    ")\n",
    "df_full_name4.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
